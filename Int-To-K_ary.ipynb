{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhingratul/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Integer to Binary Generator\n",
    "def getKary(x, b, padding):\n",
    "    assert(x >= 0)\n",
    "    assert(1< b < 37)\n",
    "    # Edge case\n",
    "    if x == 0:\n",
    "        return [0] * (padding)\n",
    "    else:\n",
    "        r = ''\n",
    "        pad = []\n",
    "        while x > 0:\n",
    "            r = string.printable[x % b] + r\n",
    "            x //= b\n",
    "            converted = list(map(int, list(r)))\n",
    "            pad = [0] * (padding - len(converted))\n",
    "            pad.extend(converted)\n",
    "        return pad\n",
    "    \n",
    "# Test case\n",
    "number = 10\n",
    "base = 3\n",
    "padding = 6\n",
    "print(getKary(number, base, padding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.0 \t  [0. 0. 1. 1. 0. 1.]\n",
      "235.0 \t  [0. 2. 2. 2. 0. 1.]\n",
      "72.0 \t  [0. 0. 2. 2. 0. 0.]\n",
      "715.0 \t  [2. 2. 2. 1. 1. 1.]\n",
      "645.0 \t  [2. 1. 2. 2. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Dataset Creation\n",
    "def create_data(num_samples, base, padding):\n",
    "    np.random.seed(1)\n",
    "    x = np.zeros(num_samples)\n",
    "    y = np.zeros((num_samples, padding))\n",
    "    max_val = base ** padding - 1\n",
    "    for i in range(num_samples):\n",
    "        number = np.random.randint(0, max_val)\n",
    "        x[i] = int(number)\n",
    "        y[i] = getKary(number, base, padding) \n",
    "    return x, y\n",
    "\n",
    "# Test Case\n",
    "num_samples = 5\n",
    "base = 3\n",
    "padding = 6\n",
    "X, y = create_data(num_samples, base, padding)\n",
    "for i in range(X.shape[0]):\n",
    "    print(X[i], '\\t ', y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Model Parameters\n",
    "padding = 4\n",
    "base = 3\n",
    "training_samples = 1000  # Need more samples to train on\n",
    "testing_samples = 20\n",
    "# Need to use adaptive LR, as the tr loss starts to diverge going forward if not used\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.01\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           10000, 0.96, staircase=True)\n",
    "training_steps = 100000  # Training longer seems to do the trick + More training samples + Multi-LSTM\n",
    "display_steps = 5000\n",
    "n_input = 1\n",
    "n_hidden_units = 32  # 32 BEST # Need more hidden units as compared to Binary to Int model\n",
    "n_output = padding\n",
    "timestep = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.0 \t [1. 1. 0. 1.] \n",
      "\n",
      "12.0 \t [0. 1. 1. 0.] \n",
      "\n",
      "72.0 \t [2. 2. 0. 0.] \n",
      "\n",
      "9.0 \t [0. 1. 0. 0.] \n",
      "\n",
      "75.0 \t [2. 2. 1. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Training and Testing Data\n",
    "X_train, y_train = create_data(training_samples, base, padding)\n",
    "X_test, y_test = create_data(testing_samples, base, padding)\n",
    "\n",
    "# Print data\n",
    "display = 5\n",
    "for i in range(display):\n",
    "    print(X_train[i], '\\t', y_train[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Model and intializations\n",
    "X = tf.placeholder(tf.float32, [None, timestep, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_output])\n",
    "W = tf.Variable(tf.random_normal([n_hidden_units, n_output]))\n",
    "b = tf.Variable(tf.random_normal([n_output]))\n",
    "\n",
    "def model(X, W, b, timestep, n_hidden_units):\n",
    "    num_layers = 3  # 3 BEST\n",
    "    dropout = tf.constant(1)  #0.7 BEST\n",
    "    X = tf.unstack(X, timestep, 1)\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(n_hidden_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout, output_keep_prob=dropout)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    lstm_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden_units),rnn.BasicLSTMCell(n_hidden_units)])  # Additional Layers Required\n",
    "    # lstm_cell = rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "    logits = tf.matmul(outputs[-1], W) + b\n",
    "    return logits\n",
    "\n",
    "logits = model(X, W, b, timestep, n_hidden_units)\n",
    "loss = tf.reduce_mean(tf.losses.mean_squared_error(logits, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  # Adam works better as compared to RMSProp\n",
    "training = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37.]] \t [1. 1. 0. 1.]\n",
      "[[12.]] \t [0. 1. 1. 0.]\n",
      "[[72.]] \t [2. 2. 0. 0.]\n",
      "[[9.]] \t [0. 1. 0. 0.]\n",
      "[[75.]] \t [2. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Reshape data\n",
    "X_train = np.reshape(X_train, [-1, timestep, n_input])\n",
    "y_train = np.reshape(y_train, [-1, n_output])\n",
    "\n",
    "X_test = np.reshape(X_test, [-1, timestep, n_input])\n",
    "y_test = np.reshape(y_test, [-1, n_output])\n",
    "\n",
    "# Print data\n",
    "display = 5\n",
    "for i in range(display):\n",
    "    print(X_train[i], '\\t', y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.57352161407 at timestep 0\n",
      "Loss 0.132245913148 at timestep 5000\n",
      "Loss 0.108583122492 at timestep 10000\n",
      "Loss 0.12410672754 at timestep 15000\n",
      "Loss 0.0780641362071 at timestep 20000\n",
      "Loss 0.0954181924462 at timestep 25000\n",
      "Loss 0.0461850613356 at timestep 30000\n",
      "Loss 0.0826843380928 at timestep 35000\n",
      "Loss 0.205419555306 at timestep 40000\n",
      "Loss 0.0687571316957 at timestep 45000\n",
      "Loss 0.0745244473219 at timestep 50000\n",
      "Loss 0.089118257165 at timestep 55000\n",
      "Loss 0.0820879340172 at timestep 60000\n",
      "Loss 0.0714473351836 at timestep 65000\n",
      "Loss 0.124975688756 at timestep 70000\n",
      "Loss 0.0915157422423 at timestep 75000\n",
      "Loss 0.0879760980606 at timestep 80000\n",
      "Loss 0.0821021646261 at timestep 85000\n",
      "Loss 0.0857841819525 at timestep 90000\n",
      "Loss 0.0788920074701 at timestep 95000\n"
     ]
    }
   ],
   "source": [
    "# Run TF\n",
    "tf.set_random_seed(0)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(training_steps):\n",
    "        _, loss_out = sess.run([training, loss], feed_dict={X: X_train, y:y_train})\n",
    "        if step % display_steps == 0:\n",
    "            print(\"Loss {} at timestep {}\" .format(loss_out, step))\n",
    "            out = sess.run(logits, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth \t Predicted\n",
      "[2. 2. 1. 1.] -> [2. 2. 1. 1.]\n",
      "[0. 1. 0. 0.] -> [0. 1. 0. 0.]\n",
      "[2. 1. 2. 2.] -> [2. 1. 2. 2.]\n",
      "[1. 0. 0. 1.] -> [1. 0. 0. 1.]\n",
      "[2. 2. 0. 0.] -> [2. 2. 0. 0.]\n",
      "[1. 1. 0. 1.] -> [1. 1. 0. 1.]\n",
      "[1. 1. 0. 1.] -> [1. 1. 0. 1.]\n",
      "[2. 2. 1. 0.] -> [2. 2. 1. 0.]\n",
      "[0. 0. 1. 2.] -> [0. 0. 1. 2.]\n",
      "[2. 2. 2. 1.] -> [2. 2. 2. 1.]\n",
      "[0. 1. 2. 1.] -> [0. 1. 2. 1.]\n",
      "[0. 1. 0. 2.] -> [0. 1. 0. 2.]\n",
      "[0. 2. 0. 2.] -> [0. 2. 0. 2.]\n",
      "[2. 2. 1. 0.] -> [2. 2. 1. 0.]\n",
      "[0. 0. 0. 1.] -> [0. 0. 0. 1.]\n",
      "[2. 2. 1. 1.] -> [2. 2. 1. 1.]\n",
      "[0. 1. 1. 0.] -> [0. 1. 1. 0.]\n",
      "[0. 1. 1. 0.] -> [0. 1. 1. 0.]\n",
      "[2. 1. 0. 1.] -> [2. 1. 0. 1.]\n",
      "[0. 0. 0. 1.] -> [0. 0. 0. 1.]\n",
      "Accuracy is 0.95 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metric\n",
    "out = np.abs(np.round(out))\n",
    "plot =  True\n",
    "if plot is True:\n",
    "    print(\"Ground Truth \\t Predicted\")\n",
    "    disp = 20\n",
    "    rdm = np.random.randint(0, y_test.shape[0], disp)\n",
    "    for i in rdm:\n",
    "        print(y_test[i], \"->\", out[i])\n",
    "acc = out == y_test\n",
    "acc = acc.sum(axis=1) == padding\n",
    "acc = acc.sum()/float(len(y_test))\n",
    "print(\"Accuracy is {} \\n\" .format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
